for distributed data parallel sweep: 

torchrun --standalone --nproc_per_node=<NUM_GPUS> sweep_params.py